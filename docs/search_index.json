[
["index.html", "W201 Portfolio Welcome! About the author", " W201 Portfolio Rutika Banakar MIDS Fall 2017 Welcome! The site showcases a couple of ideas/problems from Rutika Banakar, a student of MIDS program at School of Information, UC Berkeley. And proposed solutions to these problems which utilizes the power of data science. Smart auto-matcher Over-the-top content (OTT) is the term used in the broadcasting and technology business reporting to refer to audio, video, and other media transmitted via the Internet as a standalone product, that is, without a cable operator or direct-broadcast satellite television systems controlling or distributing the content. On a daily basis my company receives catalogs of the TV programs from various OTT providers that must be matched to the programs in the company’s database. The data my company maintains is an industry standard and the matched content is sold back to the customers who rely on it to power their applications. We can leverage the power of data science and machine learning to auto match the incoming data with our database and hence reduce time and oney spent on hand-matching these records. The devastation from hurricanes Harvey, Irma, and Maria and the dozens of wildfires that raged across West U.S. in 2017 have resulted in one of the costliest string of weather events in U.S. history. According to early estimates, over the course of few weeks, the hurricanes and wildfires have left a trail of damage that could add up to nearly $300 billion. The evidence of climate change is undeniable and offsetting your CO2 emissions is a practical and immediate way to take ownership of your personal contribution to climate change. I wish to build a tool for every environmentally cautious person to help them monitor their carbon footprint and reduce it. Small changes from everyone can add up to great benefits. About the author Figure 1: Rutika Banakar Rutika graduated with a Bachelors degree in Computer Science and Engineering from Vishweshwaraya Technological University, India in 2011 and currently works as a Software Engineer at Gracenote, a Nielsen company in Emeryville, CA. When not working she loves to bake and hike around the many awesome trails in bay area. Updated: 2017-11-22 "],
["auto-matching.html", "Better auto-matcher for matching unstructured programs data to structured program data Introduction Matching Summary and future work", " Better auto-matcher for matching unstructured programs data to structured program data Keywords matching, auto-matching, structured data, unstructured data, tv programs Introduction Over-the-top content (OTT) is the term used in the broadcasting and technology business reporting to refer to audio, video, and other media transmitted via the Internet as a standalone product, that is, without a cable operator or direct-broadcast satellite television systems controlling or distributing the content. On a daily basis my company receives catalogs of the TV programs from various OTT providers that must be matched to the programs in the company’s database. The data my company maintains is an industry standard and the matched content is sold back to the customers who rely on it to power their applications. The problem of matching records has been studied under various approaches like de-duplication, record linkage, entity resolution, etc. Matching We have a database of structured program details which is the single source of truth. Every structured record (s ∈ S) consists of a set of attributes which can be numeric or categorical in nature, like title, runtime, airdate, language, season, and episode number. We receive as input a fairly unstructured program metadata (u) and sometimes with missing data for some of the attributes. We try to find a program from our database that has the highest probability of being a match to the incoming program. The matcher is trained using a training set, U, which consists of incoming programs that have been matched to one structured record/program. We also have mismatched records from S for every u ∈ U, i.e., the things the incoming program shouldn’t match to. The correct matches are labeled as 1 and the incorrect matches are labeled as 0. Similarity feature vectors The similarity between an incoming program and the source program is defined in terms of the similarity values of the attributes present in them. For each pair of (u, s) we generate a similarity feature vector by comparing the attributes of u with the attributes of s and come up with a score between 0 and 1 for each attribute. For comparing string attributes like titles, we use string edit distance. For airdate and runtime we measure how close the two values are. For language, season number and episode number, the score is either 1 if they match exactly or 0 if they don’t match exactly. The final matching model should be able to provide a probabilistic score of the match between an incoming program and the source program. Further, the model needs to learn the relative importance of the attributes for future modeling work. The logistic regression algorithm seems to satisfy this criteria. The logistic regression learns to match from the similarity feature vectors to a binary label y, through the binary logistic function. We pair each incoming program with a small number of mismatching programs other than the correct match to introduce variability. Some of the mismatches are totally different and some of them are overlapping, like episodes from different seasons of the same show, for example. The learned matching model can then find the best match by pairing u with a set of candidates s from S (obtained by a fuzzy search on the database using only titles) and choosing the s that results in the highest score. Summary and future work The proposed solution takes into account matches as well as mismatches between the program attributes and learns to infer relative importance of different attributes in the values. The solution works well when all of the data or attributes expected for a program are present. But more often than we’d like, a lot of data is missing. The system should be taught to treat missing data differently than the mismatching data. This is something we’d like to include in future improvements. In addition, our team will explore some alternate solutions using neural networks rather than regressors. While the solution proposed here is in the context of TV programs, it applies to any domain where there is an idea of an offer and a repository of structured data. Take for example an e-commerce site trying to sell a laptop. The site gets offers from many vendors and they need to match these offers (?) to their own data in order to show different offers to the customers. In the future, we’d like to test the system with such different domains. "],
["low-carbon-footprint-homes.html", "Tool to help us reduce our carbon footprints Climate change is real How my tool will help Conclusion", " Tool to help us reduce our carbon footprints Keywords climate change, global warming, increasing temperatures Climate change is real The devastation from hurricanes Harvey, Irma, and Maria and the dozens of wildfires that raged across West U.S. have resulted in one of the costliest string of weather events in U.S. history. According to early estimates, over the course of few weeks, the hurricanes and wildfires have left a trail of damage that could add up to nearly $300 billion. The evidence of climate change is undeniable and offsetting your CO2 emissions is a practical and immediate way to take ownership of your personal contribution to climate change. This is where my application/tool will come in. My tool is for every environmentally cautious person to help them monitor their carbon footprint and reduce it. Small changes from everyone can add up to great benefits. How my tool will help The first step into reducing a person’s or a family’s carbon footprint will be to learn about their current habits and then make suggestions for improvement in the areas of food habits, energy consumptions, fuel consumption, and reusage and recycling habits. The current habits of the app user will be monitored for a few weeks and following data will be collected for a person or a family to start with – size of the family, type and amount of energy they consume, recycling habits, diet types, their main mode of commute, type and number of vehicles they own. After anaylzing the user’s data, personalized suggestions will be made to help reduce their carbon footprint. Some examples of such suggestions: 1. Alternatives to driving – walk or ride bike to work for a couple of days in a week; Carpool; Switch to low carbon vehicle; Combine errands to make fewer trips. 2. Eat locally-produced and organic food; Cut meat consumption or become a weekday vegetarian. 3. Switch to clean energy sources; be mindful of water and power consumption 4. Reuse and recycle; Make simple changes like carry a reusable grocery bag; Switch to reusable food containers and reusable produce bags. These are just a few examples and depending on a person’s habits, these changes could be personalized. Conclusion There’s certainly scope for improvements to this approach. And as part of future work we can get feedback on the changes the users have made to their daily lives and alter the suggestions accordingly. I agree that people may not be able carry out every single suggestion made by the app, but I believe baby steps could be the best way towards reducing emissions and global warming instead of trying to strike a huge bargain among world powers. Incremental steps can make serious progress towards addressing the causes of climate change. And I strongly believe that my tool can help with this cause. "]
]
